{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 05:34:53.763313: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib\n",
      "2023-10-05 05:34:53.763384: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib\n",
      "2023-10-05 05:34:53.763389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_069\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import MetricCollection, MeanSquaredError\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('/kaggle/src')\n",
    "from utils.set_seed import seed_base_torch\n",
    "from utils.feature_contena import Features\n",
    "from utils.lightning_utils import MyLightningDataModule\n",
    "\n",
    "PACKAGE_DIR = Path(\"/kaggle/src\")\n",
    "CFG = yaml.safe_load(open(PACKAGE_DIR / \"config.yaml\", \"r\"))\n",
    "print(CFG[\"wavenet\"][\"execution\"][\"exp_id\"])\n",
    "\n",
    "CFG[\"output_dir\"] = f\"/kaggle/output/{CFG['wavenet']['execution']['exp_id']}\"\n",
    "!rm -r {CFG[\"output_dir\"]}\n",
    "os.makedirs(CFG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "seed_base_torch(CFG[\"env\"][\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "NNBATCHSIZE = 64\n",
    "GROUP_BATCH_SIZE = 4000\n",
    "SEED = 321\n",
    "LR = 0.001\n",
    "SPLITS = 5\n",
    "\n",
    "ENMO_MEAN = 0.041315\n",
    "ANGLEZ_MEAN = -8.810453\n",
    "ENMO_STD = 0.101829\n",
    "ANGLEZ_STD = 35.521877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [01:01<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64107\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>anglez</th>\n",
       "      <th>enmo</th>\n",
       "      <th>event</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-05T11:15:00-0500</td>\n",
       "      <td>2.747173</td>\n",
       "      <td>-0.405729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-05T11:15:05-0500</td>\n",
       "      <td>2.747199</td>\n",
       "      <td>-0.405729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-02-05T11:15:10-0500</td>\n",
       "      <td>2.747283</td>\n",
       "      <td>-0.405729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-02-05T11:15:15-0500</td>\n",
       "      <td>2.747182</td>\n",
       "      <td>-0.405729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-02-05T11:15:20-0500</td>\n",
       "      <td>2.747308</td>\n",
       "      <td>-0.405729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>3995</td>\n",
       "      <td>2018-02-05T16:47:55-0500</td>\n",
       "      <td>-1.019410</td>\n",
       "      <td>-0.152363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>3996</td>\n",
       "      <td>2018-02-05T16:48:00-0500</td>\n",
       "      <td>-1.153043</td>\n",
       "      <td>-0.029608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>3997</td>\n",
       "      <td>2018-02-05T16:48:05-0500</td>\n",
       "      <td>-0.906871</td>\n",
       "      <td>-0.194591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>3998</td>\n",
       "      <td>2018-02-05T16:48:10-0500</td>\n",
       "      <td>-0.871929</td>\n",
       "      <td>-0.250567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>3999</td>\n",
       "      <td>2018-02-05T16:48:15-0500</td>\n",
       "      <td>-0.921470</td>\n",
       "      <td>-0.196555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         series_id  step                 timestamp    anglez      enmo event  \\\n",
       "0     af91d9a50547     0  2018-02-05T11:15:00-0500  2.747173 -0.405729   NaN   \n",
       "1     af91d9a50547     1  2018-02-05T11:15:05-0500  2.747199 -0.405729   NaN   \n",
       "2     af91d9a50547     2  2018-02-05T11:15:10-0500  2.747283 -0.405729   NaN   \n",
       "3     af91d9a50547     3  2018-02-05T11:15:15-0500  2.747182 -0.405729   NaN   \n",
       "4     af91d9a50547     4  2018-02-05T11:15:20-0500  2.747308 -0.405729   NaN   \n",
       "...            ...   ...                       ...       ...       ...   ...   \n",
       "3995  af91d9a50547  3995  2018-02-05T16:47:55-0500 -1.019410 -0.152363   NaN   \n",
       "3996  af91d9a50547  3996  2018-02-05T16:48:00-0500 -1.153043 -0.029608   NaN   \n",
       "3997  af91d9a50547  3997  2018-02-05T16:48:05-0500 -0.906871 -0.194591   NaN   \n",
       "3998  af91d9a50547  3998  2018-02-05T16:48:10-0500 -0.871929 -0.250567   NaN   \n",
       "3999  af91d9a50547  3999  2018-02-05T16:48:15-0500 -0.921470 -0.196555   NaN   \n",
       "\n",
       "      target  fold  \n",
       "0          1     3  \n",
       "1          1     3  \n",
       "2          1     3  \n",
       "3          1     3  \n",
       "4          1     3  \n",
       "...      ...   ...  \n",
       "3995       1     3  \n",
       "3996       1     3  \n",
       "3997       1     3  \n",
       "3998       1     3  \n",
       "3999       1     3  \n",
       "\n",
       "[4000 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob(f\"{CFG['dataset']['step_csv_dir']}/*.csv\")\n",
    "cv_split = pd.read_csv(CFG['dataset']['cv_split_path'])\n",
    "dfs = []\n",
    "for f in tqdm(files):\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "    # normalize\n",
    "    df[\"enmo\"] = (df[\"enmo\"] - ENMO_MEAN) / ENMO_STD\n",
    "    df[\"anglez\"] = (df[\"anglez\"] - ANGLEZ_MEAN) / ANGLEZ_STD\n",
    "\n",
    "    sid = df[\"series_id\"].values[0]\n",
    "    df[\"fold\"] = cv_split.loc[cv_split[\"series_id\"] == sid, \"fold\"].values[0]\n",
    "    for start in range(0, len(df), GROUP_BATCH_SIZE // 2):\n",
    "        end = start + GROUP_BATCH_SIZE\n",
    "        if end > len(df):\n",
    "            end = len(df)\n",
    "            start = end - GROUP_BATCH_SIZE\n",
    "            assert start >= 0\n",
    "        dfs.append(df.iloc[start: end])\n",
    "gc.collect()\n",
    "print(len(dfs))\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZzzDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dfs: list[pd.DataFrame], mode: str, features: Features):\n",
    "        self.dfs = dfs\n",
    "        self.mode = mode\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dfs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        df = self.dfs[index]\n",
    "\n",
    "        feats = df[self.features.all_features()].values.astype(np.float32)\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            targets = df[\"target\"].values.astype(np.float32)\n",
    "            return feats, targets\n",
    "        else:\n",
    "            return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.torch_models import WaveBlock    \n",
    "from utils.lightning_utils import MyLightningDataModule\n",
    "    \n",
    "class WaveNetModel(pl.LightningModule):\n",
    "    def __init__(self, dim_input: int, loss_fn=nn.CrossEntropyLoss(), lr=0.001, weight_decay=0):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay    \n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        input_size = 128\n",
    "        self.LSTM1 = nn.GRU(input_size=dim_input, hidden_size=64, num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.LSTM = nn.GRU(input_size=input_size, hidden_size=64, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        #self.attention = Attention(input_size,4000)\n",
    "        #self.rnn = nn.RNN(input_size, 64, 2, batch_first=True, nonlinearity='relu')\n",
    "               \n",
    "        self.wave_block1 = WaveBlock(128, 16, 4)\n",
    "        self.wave_block2 = WaveBlock(16, 32, 4)\n",
    "        self.wave_block3 = WaveBlock(32, 64, 2)\n",
    "        self.wave_block4 = WaveBlock(64, 128, 1)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "        \n",
    "        self.train_metrics = MetricCollection([], prefix=\"\")\n",
    "        self.valid_metrics = MetricCollection([], prefix=\"val_\")\n",
    "        \n",
    "        self.val_step_outputs = []\n",
    "        self.val_step_labels = []\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        slen = x.shape[1]\n",
    "\n",
    "        x,_ = self.LSTM1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "      \n",
    "        x = self.wave_block1(x)\n",
    "        x = self.wave_block2(x)\n",
    "        x = self.wave_block3(x)\n",
    "        \n",
    "        #x,_ = self.LSTM(x)\n",
    "        x = self.wave_block4(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x,_ = self.LSTM(x)\n",
    "        #x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        #x = self.rnn(x)\n",
    "        #x = self.attention(x)\n",
    "        x = self.fc(x)\n",
    "        logits = x.view(bs, slen)\n",
    "        return logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=3, verbose=True)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        preds = self.forward(X)\n",
    "\n",
    "        loss = self.loss_fn(preds, y)\n",
    "\n",
    "        self.train_metrics(preds, y)\n",
    "        self.log(\n",
    "            \"loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        self.log_dict(\n",
    "            self.train_metrics,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        preds = self.forward(X)\n",
    "\n",
    "        self.val_step_outputs.append(preds)\n",
    "        self.val_step_labels.append(y)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        preds = torch.cat(self.val_step_outputs)\n",
    "        labels = torch.cat(self.val_step_labels)\n",
    "        self.val_step_outputs.clear()\n",
    "        self.val_step_labels.clear()\n",
    "        loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        self.valid_metrics(preds, labels)\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "        )\n",
    "        self.log_dict(\n",
    "            self.valid_metrics,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "        )\n",
    "\n",
    "        # ログをprint\n",
    "        self.print_metric(preds, labels, \"valid\")\n",
    "\n",
    "    def print_metric(self, y_hat, y, train_or_valid=\"train\"):\n",
    "        \"\"\"\n",
    "        ログをprintする。次のepochが終わると上書きされてしまうので。\n",
    "        TODO: たぶんもっとマシな方法があるので探す。\n",
    "        \"\"\"\n",
    "        if train_or_valid == \"train\":\n",
    "            metrics = self.train_metrics\n",
    "        else:\n",
    "            metrics = self.valid_metrics\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "\n",
    "        print(f\"[epoch {self.trainer.current_epoch}] {train_or_valid}: \", end=\"\")\n",
    "        print(f\"{type(self.loss_fn).__name__}={loss:.4f}\", end=\", \")\n",
    "        for name in metrics:\n",
    "            v = metrics[name](y_hat, y)\n",
    "            print(f\"{name}={v:.4f}\", end=\", \")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features()\n",
    "features.add_num_features([\"anglez\", \"enmo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'logs': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== fold 0 ==\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a1403fd88d4f4fa18f6e65084cad54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] valid: BCEWithLogitsLoss=0.7090, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4a9184199a4a96bf46935db140f7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42621ba2b0342fea137906caaaba3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] valid: BCEWithLogitsLoss=0.1515, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73584e60e643455ea03c7a29d6b50812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] valid: BCEWithLogitsLoss=0.1205, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f2ea4cf2ba46eab067a93a211350b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] valid: BCEWithLogitsLoss=0.1179, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67a304f3f8b47899d26e827ccd22326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] valid: BCEWithLogitsLoss=0.1168, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86c446ccf8e4d13a6453e7f817661b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] valid: BCEWithLogitsLoss=0.1163, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411f9099df444f53bb4ba38caedcd8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] valid: BCEWithLogitsLoss=0.1159, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1a888a388a4e47b4136e411dc046d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6] valid: BCEWithLogitsLoss=0.1098, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3f79e71f174960af3f4b6e942637bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7] valid: BCEWithLogitsLoss=0.1270, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c521b9a736ca41b08219e5d351a98d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 8] valid: BCEWithLogitsLoss=0.1146, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3fd32a5c9141d9a8177b345103f3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 9] valid: BCEWithLogitsLoss=0.1129, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2dad3a152d340b1b3475049e5ff5190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 10] valid: BCEWithLogitsLoss=0.1149, \n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6a8d961a9f450eaf0f9ec97bfd7ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 11] valid: BCEWithLogitsLoss=0.1095, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e5a398802f41c3825d3b9aaa014584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 12] valid: BCEWithLogitsLoss=0.1102, \n"
     ]
    }
   ],
   "source": [
    "fold_oof_dfs = []\n",
    "for fold in range(5):\n",
    "    print(f\"== fold {fold} ==\")\n",
    "    \n",
    "    # 学習・評価データ\n",
    "    train_dfs = [df for df in dfs if df[\"fold\"].unique()[0] != fold]\n",
    "    valid_dfs = [df for df in dfs if df[\"fold\"].unique()[0] == fold]\n",
    "    train_dataset = ZzzDataset(train_dfs, mode=\"train\", features=features)\n",
    "    valid_dataset = ZzzDataset(valid_dfs, mode=\"train\", features=features)\n",
    "    data_module = MyLightningDataModule(train_dataset, valid_dataset, batch_size=NNBATCHSIZE)\n",
    "\n",
    "    # モデル\n",
    "    model = WaveNetModel(lr=LR, dim_input=len(features.all_features()), loss_fn=nn.BCEWithLogitsLoss())\n",
    "    \n",
    "    # コールバック\n",
    "    cp_callback = ModelCheckpoint(\n",
    "        \"logs/\", \n",
    "        filename=f\"best_model_fold{fold}\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        save_last=False,\n",
    "    )\n",
    "    es_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=5,\n",
    "    )\n",
    "\n",
    "    # 学習\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[cp_callback, es_callback],\n",
    "        )\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # 推論\n",
    "    preds = []\n",
    "    model = WaveNetModel.load_from_checkpoint(f\"logs/best_model_fold{fold}.ckpt\", dim_impput=len(features.all_features()), lr=0.001, loss_fn=nn.BCEWithLogitsLoss()).to(\"cuda\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_module.val_dataloader():\n",
    "            pred = F.sigmoid(model(X.to(\"cuda\"))).detach().cpu().numpy()\n",
    "            preds.append(pred)\n",
    "\n",
    "    oof_dfs = []\n",
    "    for pred, df in zip(np.vstack(preds), valid_dfs):\n",
    "        df[\"oof\"] = pred\n",
    "        df = df.drop(columns=features.all_features())\n",
    "        oof_dfs.append(df)\n",
    "\n",
    "    oof_df = pd.concat(oof_dfs)\n",
    "    oof_df = oof_df.groupby([\"series_id\", \"step\"]).mean().reset_index().sort_values([\"series_id\", \"step\"])\n",
    "    fold_oof_dfs.append(oof_df)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
