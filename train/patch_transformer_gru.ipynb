{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working\n",
    "%rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 21:36:04.165182: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib\n",
      "2023-10-27 21:36:04.166087: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib\n",
      "2023-10-27 21:36:04.166093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_105\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import MetricCollection, MeanSquaredError\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "sys.path.append('/kaggle/src')\n",
    "from utils.metric import compute_comptetition_metric\n",
    "from utils.set_seed import seed_base_torch\n",
    "from utils.feature_contena import Features\n",
    "from utils.lightning_utils import MyLightningDataModule\n",
    "from utils.postprocess import dynamic_range_nms\n",
    "from multiprocessing import Pool\n",
    "from consts import ANGLEZ_MEAN, ANGLEZ_STD, ENMO_MEAN, ENMO_STD\n",
    "from torch_model.dataset import ZzzPatchAugmentationDataset\n",
    "from torch_model.gru_model import ZzzTransformerGRUModule\n",
    "\n",
    "MODEL_NAME = \"patch_model\"\n",
    "\n",
    "PACKAGE_DIR = Path(\"/kaggle/src\")\n",
    "CFG = yaml.safe_load(open(PACKAGE_DIR / \"config.yaml\", \"r\"))\n",
    "print(CFG[MODEL_NAME][\"execution\"][\"exp_id\"])\n",
    "\n",
    "CFG[\"output_dir\"] = f\"/kaggle/output/{CFG[MODEL_NAME]['execution']['exp_id']}\"\n",
    "!rm -r {CFG[\"output_dir\"]}\n",
    "os.makedirs(CFG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "seed_base_torch(CFG[\"env\"][\"seed\"])\n",
    "\n",
    "BLOCK_SIZE = CFG[MODEL_NAME][\"execution\"][\"block_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [07:09<00:00,  1.55s/it]   \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>step</th>\n",
       "      <th>anglez</th>\n",
       "      <th>enmo</th>\n",
       "      <th>event</th>\n",
       "      <th>target</th>\n",
       "      <th>onset_target</th>\n",
       "      <th>wakeup_target</th>\n",
       "      <th>fold</th>\n",
       "      <th>anglez_diff</th>\n",
       "      <th>enmo_diff</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c3072a759efb</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.217345</td>\n",
       "      <td>-0.123969</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c3072a759efb</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.216284</td>\n",
       "      <td>-0.035560</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.088409</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c3072a759efb</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.121312</td>\n",
       "      <td>-0.124951</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>-0.089391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c3072a759efb</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.117824</td>\n",
       "      <td>-0.128880</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>-0.003929</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c3072a759efb</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.117486</td>\n",
       "      <td>-0.131827</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>-0.002947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      series_id  step    anglez      enmo event  target  onset_target  \\\n",
       "0  c3072a759efb     0 -2.217345 -0.123969  None       1           0.0   \n",
       "1  c3072a759efb     1 -2.216284 -0.035560  None       1           0.0   \n",
       "2  c3072a759efb     2 -2.121312 -0.124951  None       1           0.0   \n",
       "3  c3072a759efb     3 -2.117824 -0.128880  None       1           0.0   \n",
       "4  c3072a759efb     4 -2.117486 -0.131827  None       1           0.0   \n",
       "\n",
       "   wakeup_target  fold  anglez_diff  enmo_diff  phase  \n",
       "0            0.0     3     0.000000   0.000000    0.0  \n",
       "1            0.0     3     0.001061   0.088409    0.0  \n",
       "2            0.0     3     0.094972  -0.089391    0.0  \n",
       "3            0.0     3     0.003488  -0.003929    0.0  \n",
       "4            0.0     3     0.000338  -0.002947    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(19200, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv splitとマージ\n",
    "cv_split = pd.read_csv(CFG['dataset']['cv_split_path'])\n",
    "\n",
    "files = glob(f\"{CFG['dataset']['step_csv_dir']}/*.parquet\")\n",
    "dfs = []\n",
    "sleeping_dfs = []\n",
    "awake_dfs = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    df = pd.read_parquet(file)\n",
    "    df = df.drop(columns=[\"timestamp\"])\n",
    "\n",
    "    # cv split\n",
    "    df[\"fold\"] = df[\"series_id\"].map(cv_split.set_index(\"series_id\")[\"fold\"])\n",
    "\n",
    "    # 標準化\n",
    "    df[\"anglez\"] = (df[\"anglez\"] - ANGLEZ_MEAN) / ANGLEZ_STD\n",
    "    df[\"enmo\"] = (df[\"enmo\"] - ENMO_MEAN) / ENMO_STD\n",
    "    df[\"anglez_diff\"] = df[\"anglez\"].diff().fillna(0)\n",
    "    df[\"enmo_diff\"] = df[\"enmo\"].diff().fillna(0)\n",
    "\n",
    "    # targetは01にする\n",
    "    df[\"onset_target\"] = df[\"onset_target\"] / 10\n",
    "    df[\"wakeup_target\"] = df[\"wakeup_target\"] / 10\n",
    "\n",
    "    # phaseごとに分割\n",
    "    df[\"phase\"] = df[\"target\"].diff().abs().fillna(0).cumsum()\n",
    "    for _, phase_df in df.groupby(\"phase\"):\n",
    "        if phase_df[\"target\"].mean() < 0.5:\n",
    "            sleeping_dfs.append(phase_df)\n",
    "        else:\n",
    "            awake_dfs.append(phase_df)\n",
    "\n",
    "    # blockごとに分割\n",
    "    df = df.sort_values(\"step\").reset_index(drop=True)\n",
    "\n",
    "    for start in range(0, len(df), BLOCK_SIZE // 8):\n",
    "        end = start + BLOCK_SIZE\n",
    "        if end > len(df):\n",
    "            # endをlen(df)未満の最大のpsの倍数にする\n",
    "            end = len(df) - len(df) % CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n",
    "            start = end - BLOCK_SIZE\n",
    "            assert start >= 0\n",
    "        assert df.iloc[start][\"step\"] % CFG[MODEL_NAME][\"execution\"][\"patch_size\"] == 0\n",
    "        dfs.append(df.iloc[start: end])\n",
    "    del df\n",
    "gc.collect()\n",
    "\n",
    "display(dfs[0].head(5))\n",
    "dfs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 48), (1600, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = Features()\n",
    "features.add_num_features([\"anglez\", \"enmo\"])\n",
    "features.add_num_features([\"anglez_diff\", \"enmo_diff\"])\n",
    "dataset_oof = ZzzPatchAugmentationDataset(\n",
    "    dfs, 'train', features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"], \n",
    "    aug=True, sleeping_dfs=sleeping_dfs, awake_dfs=awake_dfs)\n",
    "feats, targets = dataset_oof[0]\n",
    "feats.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'logs': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== fold 0 ==\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab35a1a76b046acaf8b333273d8953d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] valid: BCEWithLogitsLoss=0.7537, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1c7947bda84fe386ad4b35ed8564de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a046f13d6b4a4c4c93c72a30f92fb6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] valid: BCEWithLogitsLoss=0.0585, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4492b06d21463c8b41ff2cfb5dfd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] valid: BCEWithLogitsLoss=0.0263, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab22c2898ead4afc979d4d045281ee61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] valid: BCEWithLogitsLoss=0.0220, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4327e6c9ce2c4896845dceabe85cf6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] valid: BCEWithLogitsLoss=0.0210, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c15e0e1102f49d9b13b6d24ef00fe7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] valid: BCEWithLogitsLoss=0.0213, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1aa36ed84344c9862b3aa3dad91bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] valid: BCEWithLogitsLoss=0.0210, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455b48fc291d40399b2e1298bb34e32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] valid: BCEWithLogitsLoss=0.0207, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a2cb5ed8534205b449047288e072a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] valid: BCEWithLogitsLoss=0.0210, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dfb57e32b3496fbbdab3732e24b9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] valid: BCEWithLogitsLoss=0.0207, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52b9bd082bc4a08a71efb3fcdbd1fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] valid: BCEWithLogitsLoss=0.0209, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e83e3940034c1a8c7fa73f7b0e3ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] valid: BCEWithLogitsLoss=0.0212, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e5d8ec568f4bf2a50be312deab5c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] valid: BCEWithLogitsLoss=0.0210, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bae3cb19e846579e6fb980f8644a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] valid: BCEWithLogitsLoss=0.0211, \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fold_oof_dfs = []\n",
    "for fold in range(5):\n",
    "    print(f\"== fold {fold} ==\")\n",
    "    \n",
    "    # 学習・評価データ\n",
    "    train_dfs = [df for df in dfs if df[\"fold\"].unique()[0] != fold]\n",
    "    valid_dfs = [df for df in dfs if df[\"fold\"].unique()[0] == fold]\n",
    "    train_sleeping_dfs = [df for df in sleeping_dfs if df[\"fold\"].unique()[0] != fold]\n",
    "    train_awake_dfs = [df for df in awake_dfs if df[\"fold\"].unique()[0] != fold]\n",
    "    train_dataset = ZzzPatchAugmentationDataset(\n",
    "        train_dfs, mode=\"train\", aug=True, features=features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n",
    "        sleeping_dfs=train_sleeping_dfs, awake_dfs=train_awake_dfs)\n",
    "    valid_dataset = ZzzPatchAugmentationDataset(valid_dfs, mode=\"train\", features=features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"])\n",
    "    data_module = MyLightningDataModule(train_dataset, valid_dataset, batch_size=32)\n",
    "\n",
    "    # モデル\n",
    "    num_training_steps = len(train_dataset) // 32 * 10\n",
    "    model = ZzzTransformerGRUModule(\n",
    "        max_len=BLOCK_SIZE // CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n",
    "        input_numerical_size=len(features.all_features()) * CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n",
    "        **CFG[MODEL_NAME][\"params\"],\n",
    "        lr=0.001, \n",
    "        dropout=0.0, \n",
    "        loss_fn=nn.BCEWithLogitsLoss(),\n",
    "        num_training_steps=num_training_steps,\n",
    "        )\n",
    "    \n",
    "    # コールバック\n",
    "    cp_callback = ModelCheckpoint(\n",
    "        \"logs/\", \n",
    "        filename=f\"best_model_fold{fold}\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        save_last=False,\n",
    "    )\n",
    "    es_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=8,\n",
    "    )\n",
    "\n",
    "    # 学習\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[cp_callback, es_callback],\n",
    "        deterministic=True,\n",
    "        val_check_interval=0.25,\n",
    "        )\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # 推論\n",
    "    model = ZzzTransformerGRUModule.load_from_checkpoint(\n",
    "        f\"logs/best_model_fold{fold}.ckpt\", \n",
    "        max_len=BLOCK_SIZE // CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n",
    "        input_numerical_size=len(features.all_features()) * CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n",
    "        **CFG[MODEL_NAME][\"params\"],\n",
    "        loss_fn=nn.BCEWithLogitsLoss()).to(\"cuda\")\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_module.val_dataloader():\n",
    "            pred = torch.sigmoid(model(X.to(\"cuda\"))).detach().cpu().numpy() * 10\n",
    "            preds.append(pred)\n",
    "\n",
    "    oof_dfs = []\n",
    "    for pred, df in zip(np.vstack(preds), valid_dfs):\n",
    "        df = df.iloc[CFG[MODEL_NAME][\"execution\"][\"patch_size\"] // 2: len(df): CFG[MODEL_NAME][\"execution\"][\"patch_size\"]].reset_index(drop=True)\n",
    "        df[[\"wakeup_oof\", \"onset_oof\"]] = pred\n",
    "        oof_dfs.append(df)\n",
    "\n",
    "    oof_df = pd.concat(oof_dfs)\n",
    "    oof_df = oof_df.groupby([\"series_id\", \"step\"]).mean().reset_index().sort_values([\"series_id\", \"step\"])\n",
    "    fold_oof_dfs.append(oof_df[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\"]])\n",
    "\n",
    "    del model, preds, oof_df, oof_dfs\n",
    "    gc.collect()\n",
    "    # break\n",
    "train = pd.concat(fold_oof_dfs)\n",
    "train.to_parquet(f\"{CFG['output_dir']}/oof.parquet\", index=False)\n",
    "del fold_oof_dfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "df = train[[\"series_id\", \"step\", \"wakeup_oof\"]].copy()\n",
    "df[\"event\"] = \"wakeup\"\n",
    "df[\"score\"] = df[\"wakeup_oof\"]\n",
    "dfs.append(df[['series_id', 'step', 'event', 'score']])\n",
    "\n",
    "df = train[[\"series_id\", \"step\", \"onset_oof\"]].copy()\n",
    "df[\"event\"] = \"onset\"\n",
    "df[\"score\"] = df[\"onset_oof\"]\n",
    "dfs.append(df[['series_id', 'step', 'event', 'score']])\n",
    "\n",
    "train = pd.concat(dfs)\n",
    "train = train[train[\"score\"]>0.1].reset_index(drop=True)\n",
    "train[\"score\"].hist()\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic-Range NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"step\"] = train[\"step\"].astype(int)\n",
    "\n",
    "groups = [group for _, group in train.groupby(\"series_id\")]\n",
    "with Pool(30) as p:  \n",
    "    results = list(tqdm(p.imap(dynamic_range_nms, groups), total=len(groups)))\n",
    "sub = pd.concat(results)\n",
    "sub[\"score\"] = sub[\"reduced_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スコア計算\n",
    "labels = pd.read_csv(f\"{CFG['dataset']['competition_dir']}/train_events.csv\").dropna()\n",
    "labels = labels[labels[\"series_id\"].isin(sub[\"series_id\"].unique())]\n",
    "score, ap_table = compute_comptetition_metric(labels, sub)\n",
    "\n",
    "print(f\"score: {score:.4f}\")\n",
    "display(ap_table)\n",
    "sub.to_csv(os.path.join(CFG[\"output_dir\"], \"submission.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp -r logs {CFG[\"output_dir\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
