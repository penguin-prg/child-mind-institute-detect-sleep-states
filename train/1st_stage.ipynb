{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 16:15:31.795349: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-10 16:15:32.230783: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-10 16:15:33.634044: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib\n",
      "2023-10-10 16:15:33.634567: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib\n",
      "2023-10-10 16:15:33.634572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_076\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('/kaggle/src')\n",
    "from utils.xgb import fit_xgb, inference_xgb, plot_importances\n",
    "from utils.metric import compute_comptetition_metric\n",
    "from utils.postprocess import post_process\n",
    "from utils.set_seed import seed_base\n",
    "from feature_engineering.stage1 import generate_1st_stage_features\n",
    "\n",
    "PACKAGE_DIR = Path(\"/kaggle/src\")\n",
    "CFG = yaml.safe_load(open(PACKAGE_DIR / \"config.yaml\", \"r\"))\n",
    "print(CFG[\"1st_stage\"][\"execution\"][\"exp_id\"])\n",
    "\n",
    "CFG[\"output_dir\"] = f\"/kaggle/output/{CFG['1st_stage']['execution']['exp_id']}\"\n",
    "!rm -r {CFG[\"output_dir\"]}\n",
    "os.makedirs(CFG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "seed_base(CFG[\"env\"][\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate features: 100%|██████████| 277/277 [06:34<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6397317, 208)\n"
     ]
    }
   ],
   "source": [
    "# 特徴生成\n",
    "files = glob(f\"{CFG['dataset']['step_csv_dir']}/*.csv\")\n",
    "train, features = generate_1st_stage_features(files, downsample_rate=CFG[\"1st_stage\"][\"execution\"][\"downsample_rate\"])\n",
    "\n",
    "# cv splitとマージ\n",
    "cv_split = pd.read_csv(CFG['dataset']['cv_split_path'])\n",
    "train[\"fold\"] = train[\"series_id\"].map(cv_split.set_index(\"series_id\")[\"fold\"])\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== fold 0 ==\n",
      "[0]\teval-logloss:0.61108\n",
      "[100]\teval-logloss:0.09822\n",
      "[200]\teval-logloss:0.09895\n",
      "[239]\teval-logloss:0.09923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:43<02:54, 43.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== fold 1 ==\n",
      "[0]\teval-logloss:0.61122\n",
      "[100]\teval-logloss:0.08163\n",
      "[200]\teval-logloss:0.08097\n",
      "[228]\teval-logloss:0.08104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [01:20<01:59, 39.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== fold 2 ==\n",
      "[0]\teval-logloss:0.61074\n",
      "[100]\teval-logloss:0.08441\n",
      "[200]\teval-logloss:0.08389\n",
      "[250]\teval-logloss:0.08400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:55<01:14, 37.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== fold 3 ==\n",
      "[0]\teval-logloss:0.61145\n",
      "[100]\teval-logloss:0.09297\n",
      "[200]\teval-logloss:0.09231\n",
      "[268]\teval-logloss:0.09236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [02:31<00:37, 37.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== fold 4 ==\n",
      "[0]\teval-logloss:0.61185\n",
      "[100]\teval-logloss:0.10486\n",
      "[200]\teval-logloss:0.10439\n",
      "[286]\teval-logloss:0.10514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:08<00:00, 37.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習\n",
    "trn_oof, models = fit_xgb(\n",
    "    X=train, \n",
    "    y=train[\"target\"], \n",
    "    folds=train[\"fold\"].astype(int), \n",
    "    features=features.all_features(),\n",
    "    params=CFG[\"1st_stage\"][\"xgboost\"], \n",
    "    es_rounds=100,\n",
    ")\n",
    "\n",
    "# 保存\n",
    "for i, model in enumerate(models):\n",
    "    model.save_model(os.path.join(CFG[\"output_dir\"], f'xgb_fold{i}.model'))\n",
    "with open(os.path.join(CFG[\"output_dir\"], \"features.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(features, f)\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [03:53<00:00, 33.35s/it]\n",
      "100%|██████████| 7/7 [03:46<00:00, 32.40s/it]\n",
      "100%|██████████| 7/7 [03:52<00:00, 33.28s/it]\n",
      "100%|██████████| 7/7 [03:50<00:00, 32.93s/it]\n",
      "100%|██████████| 7/7 [04:00<00:00, 34.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# 推論\n",
    "dfs = []\n",
    "for fold, fold_df in cv_split.groupby(\"fold\"):\n",
    "    fold_df.reset_index(drop=True, inplace=True)\n",
    "    stride = 8\n",
    "    for start in tqdm(range(0, len(fold_df), stride)):\n",
    "        end = min(start + stride, len(fold_df))\n",
    "        series_ids = fold_df.iloc[start:end][\"series_id\"].values\n",
    "        files = [f\"{CFG['dataset']['step_csv_dir']}/{series_id}.csv\" for series_id in series_ids]\n",
    "\n",
    "        # 特徴生成\n",
    "        train, features = generate_1st_stage_features(files, pbar=False)\n",
    "\n",
    "        # 推論\n",
    "        model = models[fold]\n",
    "        preds = inference_xgb([model], train[features.all_features()])\n",
    "        train[\"oof\"] = preds\n",
    "        train.drop(columns=features.all_features(), inplace=True)\n",
    "        dfs.append(train)\n",
    "\n",
    "train = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "del dfs\n",
    "gc.collect()\n",
    "\n",
    "# oofの保存\n",
    "train = train[[\"series_id\", \"step\", \"oof\", \"target\"]]\n",
    "train.to_parquet(os.path.join(CFG[\"output_dir\"], \"oof.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aeaccc01c4440f1bc38739017d1fcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing series:   0%|          | 0/277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.5224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event   tolerance\n",
       "onset   12           0.008861\n",
       "        36           0.091589\n",
       "        60           0.267211\n",
       "        90           0.477007\n",
       "        120          0.593797\n",
       "        150          0.647902\n",
       "        180          0.677465\n",
       "        240          0.707337\n",
       "        300          0.734636\n",
       "        360          0.750056\n",
       "wakeup  12           0.021398\n",
       "        36           0.193058\n",
       "        60           0.371167\n",
       "        90           0.532530\n",
       "        120          0.632525\n",
       "        150          0.686993\n",
       "        180          0.715681\n",
       "        240          0.752416\n",
       "        300          0.781686\n",
       "        360          0.804758\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 後処理\n",
    "sub = post_process(train)\n",
    "sub.to_csv(os.path.join(CFG[\"output_dir\"], \"submission.csv\"), index=False)\n",
    "\n",
    "# スコア計算\n",
    "labels = pd.read_csv(f\"{CFG['dataset']['competition_dir']}/train_events.csv\").dropna()\n",
    "score, ap_table = compute_comptetition_metric(labels, sub)\n",
    "print(f\"score: {score:.4f}\")\n",
    "display(ap_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 次の候補を決定\n",
    "# next_cand_size = 0\n",
    "# count = 0\n",
    "# next_dict = {}\n",
    "# for series_id, train_df in train.groupby(\"series_id\"):\n",
    "#     train_df = train_df[(train_df[\"oof\"] >= 0.1) & (train_df[\"oof\"] <= 0.9)]\n",
    "#     sub_df = sub[(sub[\"series_id\"] == series_id)]\n",
    "#     label_df = labels[labels[\"series_id\"] == series_id]\n",
    "#     pred_steps = train_df[\"step\"].values\n",
    "#     sub_steps = sub_df[\"step\"].values\n",
    "#     label_steps = label_df[\"step\"].values\n",
    "\n",
    "#     if len(train_df) == 0:\n",
    "#         continue\n",
    "#     next_cand = np.zeros(int(max(max(pred_steps if len(pred_steps) > 0 else [0]), max(sub_steps if len(sub_steps) > 0 else [0]))) + CFG[\"feature\"][\"agg_freq\"])\n",
    "#     for sub_step in sub_steps:\n",
    "#         next_cand[int(sub_step - CFG[\"feature\"][\"agg_freq\"] * 10): int(sub_step + CFG[\"feature\"][\"agg_freq\"] * 10)] = 1\n",
    "#     for pred_step in pred_steps:\n",
    "#         next_cand[int(pred_step - CFG[\"feature\"][\"agg_freq\"] * 10): int(pred_step + CFG[\"feature\"][\"agg_freq\"] * 10)] = 1\n",
    "#     next_cand_size += np.sum(next_cand)\n",
    "#     next_dict[series_id] = np.where(next_cand)[0]\n",
    "\n",
    "#     for label_step in label_steps:\n",
    "#         if label_step < next_cand.shape[0]:\n",
    "#             count += next_cand[int(label_step)]\n",
    "    \n",
    "# recall = count / len(labels)\n",
    "# print(f\"recall: {recall:.4f}\")\n",
    "# print(f\"next_cand_size: {next_cand_size}\")\n",
    "\n",
    "# with open(f\"{CFG['output_dir']}/next_cands.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(next_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = plot_importances(models, save_path=f\"{CFG['output_dir']}/importances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
