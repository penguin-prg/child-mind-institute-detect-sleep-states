{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working\n",
    "%rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 05:50:17.343985: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib\n",
      "2023-10-15 05:50:17.344192: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib\n",
      "2023-10-15 05:50:17.344198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_085\n",
      "rm: cannot remove '/kaggle/output/exp_085': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import MetricCollection, MeanSquaredError\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "sys.path.append('/kaggle/src')\n",
    "from utils.metric import compute_comptetition_metric\n",
    "from utils.set_seed import seed_base_torch\n",
    "from utils.feature_contena import Features\n",
    "from utils.lightning_utils import MyLightningDataModule\n",
    "from utils.postprocess import dynamic_range_nms\n",
    "from multiprocessing import Pool\n",
    "from consts import ANGLEZ_MEAN, ANGLEZ_STD, ENMO_MEAN, ENMO_STD\n",
    "from torch_model.dataset import ZzzDataset\n",
    "from torch_model.gru_model import ZzzGRUModule\n",
    "\n",
    "PACKAGE_DIR = Path(\"/kaggle/src\")\n",
    "CFG = yaml.safe_load(open(PACKAGE_DIR / \"config.yaml\", \"r\"))\n",
    "print(CFG[\"gru\"][\"execution\"][\"exp_id\"])\n",
    "\n",
    "CFG[\"output_dir\"] = f\"/kaggle/output/{CFG['gru']['execution']['exp_id']}\"\n",
    "!rm -r {CFG[\"output_dir\"]}\n",
    "os.makedirs(CFG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "seed_base_torch(CFG[\"env\"][\"seed\"])\n",
    "\n",
    "SEQ_LEN = CFG[\"gru\"][\"execution\"][\"seq_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [01:23<00:00,  3.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3118"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob(f\"{CFG['dataset']['step_csv_dir']}/*.csv\")\n",
    "dfs = []\n",
    "for file in tqdm(files):\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "train = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "del dfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化\n",
    "train[\"anglez\"] = (train[\"anglez\"] - ANGLEZ_MEAN) / ANGLEZ_STD\n",
    "train[\"enmo\"] = (train[\"enmo\"] - ENMO_MEAN) / ENMO_STD\n",
    "train[\"anglez_diff\"] = train[\"anglez\"].diff().fillna(0)\n",
    "train[\"enmo_diff\"] = train[\"enmo\"].diff().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>anglez</th>\n",
       "      <th>enmo</th>\n",
       "      <th>event</th>\n",
       "      <th>target</th>\n",
       "      <th>onset_target</th>\n",
       "      <th>wakeup_target</th>\n",
       "      <th>anglez_diff</th>\n",
       "      <th>enmo_diff</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-05T11:15:00-0500</td>\n",
       "      <td>2.747306</td>\n",
       "      <td>-0.405894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-05T11:15:05-0500</td>\n",
       "      <td>2.747331</td>\n",
       "      <td>-0.405894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-02-05T11:15:10-0500</td>\n",
       "      <td>2.747416</td>\n",
       "      <td>-0.405894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-02-05T11:15:15-0500</td>\n",
       "      <td>2.747314</td>\n",
       "      <td>-0.405894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>af91d9a50547</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-02-05T11:15:20-0500</td>\n",
       "      <td>2.747441</td>\n",
       "      <td>-0.405894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      series_id  step                 timestamp    anglez      enmo event  \\\n",
       "0  af91d9a50547     0  2018-02-05T11:15:00-0500  2.747306 -0.405894   NaN   \n",
       "1  af91d9a50547     1  2018-02-05T11:15:05-0500  2.747331 -0.405894   NaN   \n",
       "2  af91d9a50547     2  2018-02-05T11:15:10-0500  2.747416 -0.405894   NaN   \n",
       "3  af91d9a50547     3  2018-02-05T11:15:15-0500  2.747314 -0.405894   NaN   \n",
       "4  af91d9a50547     4  2018-02-05T11:15:20-0500  2.747441 -0.405894   NaN   \n",
       "\n",
       "   target  onset_target  wakeup_target  anglez_diff  enmo_diff  fold  \n",
       "0       1           0.0            0.0     0.000000        0.0     3  \n",
       "1       1           0.0            0.0     0.000025        0.0     3  \n",
       "2       1           0.0            0.0     0.000084        0.0     3  \n",
       "3       1           0.0            0.0    -0.000101        0.0     3  \n",
       "4       1           0.0            0.0     0.000127        0.0     3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(127946340, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv splitとマージ\n",
    "cv_split = pd.read_csv(CFG['dataset']['cv_split_path'])\n",
    "train[\"fold\"] = train[\"series_id\"].map(cv_split.set_index(\"series_id\")[\"fold\"])\n",
    "display(train.head(5))\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [00:31<00:00,  8.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000, 4), (10000, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SEQ_LEN ごとに分割\n",
    "dfs = []\n",
    "for series_id, df in tqdm(train.groupby(\"series_id\")):\n",
    "    df = df.sort_values(\"step\")\n",
    "\n",
    "    for start in range(0, len(df), SEQ_LEN // 2):\n",
    "        end = start + SEQ_LEN\n",
    "        if end > len(df):\n",
    "            end = len(df)\n",
    "            start = end - SEQ_LEN\n",
    "            assert start >= 0\n",
    "        dfs.append(df.iloc[start: end])\n",
    "gc.collect()\n",
    "\n",
    "features = Features()\n",
    "features.add_num_features([\"anglez\", \"enmo\"])\n",
    "features.add_num_features([\"anglez_diff\", \"enmo_diff\"])\n",
    "dataset_oof = ZzzDataset(dfs, 'train', features)\n",
    "feats, targets = dataset_oof[0]\n",
    "feats.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'logs': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== fold 0 ==\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9c4c2db2d5496396e574a0cba605c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] valid: MSELoss=0.7172, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0b1b221211441e98ec1f4a0959045f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd3433afb43497c8f4363ef088fb544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] valid: MSELoss=0.2171, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6042d6924e4414e8dbee91b071abd3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] valid: MSELoss=0.1833, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc9d6b55a454501a51d1bc72e864c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] valid: MSELoss=0.1773, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c49f599aea5455fa827c8aa1bc1cfc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] valid: MSELoss=0.1712, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3ba579e3264bf58ae84a7c644ed96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] valid: MSELoss=0.1628, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a50c3f0a4854415a4c88af99b9b626a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] valid: MSELoss=0.1684, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26b1e65c3e443e3b7edda35de39debc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6] valid: MSELoss=0.1628, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f359e3be2a24a1bb1309199797c7042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7] valid: MSELoss=0.1638, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8695c34c64ef4acfb03117b98d9ed037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 8] valid: MSELoss=0.1651, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792a5728a6e54b2e8afaf1e9d47e523e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 9] valid: MSELoss=0.1704, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678ad2a3cdf54bdfaa672e69f6c38d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 10] valid: MSELoss=0.1760, \n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fae030a9284463a9fb777b0b5f0652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 11] valid: MSELoss=0.1739, \n",
      "== fold 1 ==\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f106505024646b1aefb997c2694447e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] valid: MSELoss=0.6308, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22c53b059544e7c97c9104a4e2f79c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08632049a6cb4cacbea133df8978a1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0] valid: MSELoss=0.1999, \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fold_oof_dfs = []\n",
    "for fold in range(5):\n",
    "    print(f\"== fold {fold} ==\")\n",
    "    \n",
    "    # 学習・評価データ\n",
    "    train_dfs = [df for df in dfs if df[\"fold\"].unique()[0] != fold]\n",
    "    valid_dfs = [df for df in dfs if df[\"fold\"].unique()[0] == fold]\n",
    "    train_dataset = ZzzDataset(train_dfs, mode=\"train\", features=features)\n",
    "    valid_dataset = ZzzDataset(valid_dfs, mode=\"train\", features=features)\n",
    "    data_module = MyLightningDataModule(train_dataset, valid_dataset, batch_size=64)\n",
    "\n",
    "    # モデル\n",
    "    model = ZzzGRUModule(lr=0.001, dropout=0.0, loss_fn=nn.MSELoss(), input_numerical_size=len(features.all_features()))\n",
    "    \n",
    "    # コールバック\n",
    "    cp_callback = ModelCheckpoint(\n",
    "        \"logs/\", \n",
    "        filename=f\"best_model_fold{fold}\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        save_last=False,\n",
    "    )\n",
    "    es_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=5,\n",
    "    )\n",
    "\n",
    "    # 学習\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[cp_callback, es_callback],\n",
    "        )\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # 推論\n",
    "    model = ZzzGRUModule.load_from_checkpoint(\n",
    "        f\"logs/best_model_fold{fold}.ckpt\", \n",
    "        input_numerical_size=len(features.all_features()),\n",
    "        loss_fn=nn.BCEWithLogitsLoss()).to(\"cuda\")\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_module.val_dataloader():\n",
    "            pred = model(X.to(\"cuda\")).detach().cpu().numpy()\n",
    "            preds.append(pred)\n",
    "\n",
    "    oof_dfs = []\n",
    "    for pred, df in zip(np.vstack(preds), valid_dfs):\n",
    "        df[[\"wakeup_oof\", \"onset_oof\"]] = pred\n",
    "        oof_dfs.append(df)\n",
    "\n",
    "    oof_df = pd.concat(oof_dfs)\n",
    "    oof_df = oof_df.groupby([\"series_id\", \"step\"]).mean().reset_index().sort_values([\"series_id\", \"step\"])\n",
    "    fold_oof_dfs.append(oof_df[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\"]])\n",
    "\n",
    "    del model, preds, oof_df, oof_dfs\n",
    "    gc.collect()\n",
    "train = pd.concat(fold_oof_dfs)\n",
    "train.to_csv(f\"{CFG['output_dir']}/oof.csv\", index=False)\n",
    "del fold_oof_dfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "df = train[[\"series_id\", \"step\", \"wakeup_oof\"]].copy()\n",
    "df[\"event\"] = \"wakeup\"\n",
    "df[\"score\"] = df[\"wakeup_oof\"]\n",
    "dfs.append(df[['series_id', 'step', 'event', 'score']])\n",
    "\n",
    "df = train[[\"series_id\", \"step\", \"onset_oof\"]].copy()\n",
    "df[\"event\"] = \"onset\"\n",
    "df[\"score\"] = df[\"onset_oof\"]\n",
    "dfs.append(df[['series_id', 'step', 'event', 'score']])\n",
    "\n",
    "train = pd.concat(dfs)\n",
    "train = train[train[\"score\"]>0.1].reset_index(drop=True)\n",
    "train[\"score\"].hist()\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic-Range NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [group for _, group in train.groupby(\"series_id\")]\n",
    "with Pool(30) as p:  \n",
    "    results = list(tqdm(p.imap(dynamic_range_nms, groups), total=len(groups)))\n",
    "sub = pd.concat(results)\n",
    "sub[\"score\"] = sub[\"reduced_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スコア計算\n",
    "labels = pd.read_csv(f\"{CFG['dataset']['competition_dir']}/train_events.csv\").dropna()\n",
    "score, ap_table = compute_comptetition_metric(labels, sub)\n",
    "\n",
    "print(f\"score: {score:.4f}\")\n",
    "display(ap_table)\n",
    "sub.to_csv(os.path.join(CFG[\"output_dir\"], \"submission.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp -r logs {CFG[\"output_dir\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
